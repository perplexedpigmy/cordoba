# C⊕rdoba: C++ versioned NoSQL Database

![C⊕rdoba Logo](img/logo.svg)

C⊕rdoba is a lightweight C++ library for managing versioned NoSQL documents. Like the Spanish city famous for its libraries, C⊕rdoba can store, update and read data, better known in the Andalusian dialet as [CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete). Unlike the city, and thanks it being powered by [**libgit2**](https://github.com/libgit2/libgit2) it can practically do anything that git can, either directly via C⊕rdoba or or if not yet supported by using the git command line, git daemon etc. i.e. Cloning, branching, remote connection, merging, history view etc.

Transactions a are also a thing, given that they are implicitly supported via commits, thusly, Atomicity and Durability are already provided. To achieve full [ACID](https://en.wikipedia.org/wiki/ACID), Consistency and Isolation can be introduced, not currently foreseen.

The library was introduced as a proof of concept for a comercial entrprise solution that required to captured and address no strucutred versioned data, with fast reads, decent store time and the ability to manage different versions. It didn't require a schema, fast queries, nor security, features that can be introduced, but are not part of current project. There is not much extra work foreseen in the shortterm  on this library, it can be used as is, or as a window to git's impressive performance as DB backend.

----

## Installation

### Using cmake installers

Lightwight cmake package managers like [CPM](https://github.com/cpm-cmake/CPM.cmake) can be used to build and use the library as a dependency

### Building from source

If you use devcontainers, there is a `.devcontainer` directory with all needed to start building the library.

Otherwise, it's as simple as cloning the project and running a cmake >= 3.14

```bash
git clone https://github.com/perplexedpigmy/cordoba
cd cordoba
cmake -DCMAKE_BUILD_TYPE=Release -S . -B build/release
cmake --build build/release
```

----

## Usage 

Assuming some package manager is used, and the linkage with the library is spoken for.
All one needs is include the header files and use the use the relevant namespace

```c++
#include <gd/gd.h>
using namespace gd; // All of C⊕rdoba's functionality 
using namespace gd::shorthand; // add >>, || chaining, optional and recommended

```

**why use `gd::shorthand`?**

The library's interface have foregone exceptions, it's using `expected`, also know as (Result in rust, Either in scalla/haskell) a return value that can return either a response or an error, While not native to c++ and missing some constructs that will help use it to it utmost glory, It's still an interesting concept to use.

It allows chaining commands. Here is how one could open a database add a file to it and commit it,
in just one line. Being assured that if an error happens in any of the calls, no harm is done,
and `ctx` will hold that error, and skip any commands that happen thereafter.

```c++
  auto ctx = selectRepository(repoPath)
    .and_then( add("README", "not\n") )
    .and_then( commit("test", "test@here.org", "feat: add README") );
```

One can even do better, we can rollback on error, after having rolledback our change.

```c++
  auto ctx = selectRepository(repoPath)
    .and_then( add("README", "not\n") )
    .and_then( commit("test", "test@here.org", "feat: add README") )
    .or_else( [](const auto& err) { 
      log(err); 
      rollback() 
    });
```
Alas the `and_then` and `or_else` can be tedious to type and a bit of clutter on the eyes. 
This is where the shorthand can come handy

```c++
using namespace gd::shorthand; 

  auto ctx = selectRepository(repoPath)
    >> add("README", "not\n") 
    >> commit("test", "test@here.org", "feat: add README")
    || [](const auto& err) { 
        log(err); 
        rollback() 
      };
```

So from now on only the shorthand is used.

### Creating a branch

Any type of command can be changed and even multiple times, in the below example 3 branches are forked from the current context. The default context when just openning a repsitory is the last commit of the `main` branch.
Creating a branch does not change the context to that branch.

```c++
  auto ctx = selectRepository(repoPath)
    >> createBranch("KenAndRitchie")
    >> createBranch("StevenPinker")
    >> createBranch("AIReboot")
    || [](const auto& err) { cout << "Failed to create branches: " << err << std::endl; };
```

The context can be reused as long as it is not containing an Error.

```c++
  auto ctx = selectRepository(repoPath)
    >> createBranch("KenAndRitchie");

  ctx >> createBranch("2nd branch");

```

But it can be also trivially recreated using `selectRepository`, once a repository is created, it's cached
and it's trivial and fast to re`select` it. However, but it's more to type and less readable.

```c++
  selectRepository(repoPath)
    >> createBranch("KenAndRitchie");

  selectRepository(repoPath)
    >> createBranch("2nd branch");
```
Okay, so we have a repository and we kind of understand branches, what's next?
We kind of saw it before, yet, this is the same think on stereoids, introducing first commit with a problem, and fixing in the next commit, all in one chained command.

```c++
  auto ctx = selectRepository(repoPath)
    >> selectBranch("StevenPinker")
    >> add("the/blank/slate", "If you think the nature-nurture debate has been resolved, you are wrong ... this book is required reading ― Literary Review")
    >> add("the/staff/of/thought", "Powerful and gripping")
    >> add("Enlightement now", "THE TOP FIVE SUNDAY TIMES BESTSELLER")
    >> commit("test", "test@here.org", "add reviews")

    >> add("Enlightement now", "THE TOP **TEN** SUNDAY TIMES BESTSELLER")
    >> commit("test", "test@here.org", "correct review")
    || [](const auto& err) { std::cout << "Failed introducing data" << err << std::endl; };
```

Failures can be caught with the `or_else` clause as shown above, but they can also be tested, allowing for more logic in between calls to the database

```c++
  auto ctx = selectRepository(repoPath) 
    >> selectBranch("KenAndRitchie");

  if (!ctx) 
    cerr << "Oops: " << ctx.error() << std::endl;
  else
    cout << "Successful switch to KenAndRitchie branch" << std::endl;
```

`expected` has `operator bool()` defined, returning false if it has an error.

There are 2 way to access file contents 

```c++
  auto ctx = selectRepository(repoPath)
    >> selectBranch("StevenPinker")
    >> read("the/blank/slate");

    if (!!ctx) // Explicitly use the read content. providing context is healthy
      cout << "The blank Slate: " << ctx->content() << endl;

    // Using closure to read the content
    ctx 
      >> read("Enlightement now")
      >> processContent([](auto content) { 
        cout << "Enlightment NOW: " << content << endl;
      })
      >> add("BookIRead.txt", "Enlightment NOW")
      >> commit("test", "test@here.org", "correct review")
      || [](const auto& err) { cout << "Oops: " <<  err << endl; };
```

That's about it, lightweight, simple and easy to use.

## Performance

There are two aspects for perfromace that I thought were interesting to measure,

### Commit size impact on performance (timing.cpp)

The first being the imapct of commit size on commit time. This is interesting because git is known to have a perfromace degradation with larger directories (bigger trees, longer look up times) but also the way libgit2 is implemented, the tree is constructed from the tip, meaning a naive implementation having multiple files introduced to the same directory will have constructed the directory structure once for each of the introduced files (or file version), which will cause an exponantioal growth.

In fact such naive implementation was introduced to gaze its performance, and identify the fastest implementation.

The tests were using file sizes of 100 random characters on my not so powerful laptop,
  model name      : 12th Gen Intel(R) Core(TM) i7-1260P
  cpu MHz         : 2241.490
  cache size      : 18432 KB
  cpu cores       : 12

with a low end SSD drive

Each commit haad growing N files added to each of 13 directories in one commit, and the entire action's time was measured. 


#### Naive implementation

| Num files | num dirs | Total files | Total Time(s) | Files/sec |
|:---------:|:--------:|:-----------:|:------------:|:----------:|
| 10        | 13       |  130        | 0.0909       |   140.15   |  
| 100       | 13       |  1,300      | 1.11         |   90.144   |
| 1,000     | 13       |  13,000     | 37.94        |   26.358   |
| 1,0000    | 13       |  130,000    | 3259.63      |   3.6794   |

And indeed the naive implementation scaled poorly with growing number of files.

### C⊕rdoba's Implemenation

| Num files | num dirs | Total files | Total Time(s)| Files/sec |
|:---------:|:--------:|:-----------:|:------------:|:----------:|
| 10        | 13       |  130        | 0.0227       |   440.15   |  
| 100       | 13       |  1,300      | 0.1463       |   683.61   |
| 1,000     | 13       |  13,000     | 1.3015       |   768.31   |
| 10,000    | 13       |  130,000    | 10.392       |   962.24   |

With 10,000 files per directory we still didn't reach the point of degradation and in fact the commit was perfroming better per second (more files).

### Concurrent perfromance

The other concern is that, git was not designed to have clients working on it in parallel and definetly not in different contexts (Branches, commits, etc), so synchronization primitives had to be introduced and the question that some may ask is what is the price to pay.

To measure that a utility was introduced i.e. 'greens' that launches `G` agents each of which will introduce `C` each of which is composed of `A` number of CRUD actions. so in essence `G x C x A` actions are happening in `G` concurrent contexts. The utility took some liberties to avoid complex merging scenarios, merging scenarios may occur of multiple agents try to add to the same branch at the same time, C⊕rdoba won't allow that only the first agent wins, the others will be notified that their context is stale, the choice is merging, or rolling back and trying again. 2nd approach was selected.

#### IMPORTANT

1. The size and content of the files are also random, so some time variation beyond scaling should be expected, but we assume they are marginal (far lower than linear).

2. The utility also validates the expected DAG against the git repository, so not all the time consumed is related to what we are interested in measuring, So we can read the numbers as some kind of worse case scenario plus some buffer, and the numbers are still fine for benchmarking.

3. Tested on a laptop, with Windows manager and plenty of apps running, varation can come from anywhere, but it's good enough, just don't stick to the numbers as set in stone.

Great, let's have some fun. and see whether it we get any scalabilitiy issues.

```sh
# 5 concurrent agents x 30 Commits x 50 actions = 7,500 Actions = 3,676.47 per second
time ./build/release/main/gd/greens -g 5 -b 10 -c 30 -a 50
Success

real    0m2.046s
user    0m1.473s
sys     0m1.637s
```

Let's draw a table with out result with num agents as our variable.

| Num agents | Num Actions |  real   |  user   |   sys   | Actions/sec |
|:----------:|:-----------:|:-------:|:-------:|:-------:|:-----------:|
|    5       |   7,500     | 2.046s  |  1.631s |  1.534s |   3,676.47  |
|    10      |  15,000     | 5.637s  |  5.077s |  4.572s |   2,660.98  |
|    20      |  30,000     | 16.479s | 15.927  | 12.750s |   1,820.50  |
|    25      |  37,500     | 28.062s | 25.179s | 22.382s |   1,336.32  |

Interesting, roughly speeking doubling the agents( Which is also doubling the actions), is tripling the time, which is mostly due to rollbacks/retries when there is a contention. One can argue whether this is a reasonable use case, but even if it is, the linear scaling is acceptable.

But let's dig digger, what if we doubled the agents but kept the same number of actions?
we can do it by pairing 80 actions for 5 agents, 40 for 10 agents etc.

| Num agents | Num Actions |  real   |  user   |   sys   | Actions/sec |
|:----------:|:-----------:|:-------:|:-------:|:-------:|:-----------:|
|    5       |  12,000     |  3.694s |  2.067s |  3.213s |   3,252.03  |
|    10      |  12,000     |  3.670s |  3.308s |  2.912s |   3,269.03  |
|    20      |  12,000     |  5.971s |  5.500s |  4.395s |   2,009.71  |

This indeed enforces the hypothesis that the contention may be indeed the culprit the degradation above.
because we don't see much effect when we doubled the agents. Only when we reached more agents than our cores did we see a noticable degradation.

lets dig dipper, what if we kept the number of agents static, and played with actions?

| Num agents | Actions/Commit |  Num Commits | Total Actions |   real  |  user   |  sys   | Actions/sec |
|:----------:|:--------------:|:------------:|:-------------:|:-------:|:-------:|:------:|:-----------:|
|    10      |  20            |  30          |   6,000       |  2.071s | 1.595s  | 1.618s |  2,897.15   |
|    10      |  40            |  30          |  12,000       |  4.200s | 3.758s  | 3.451s |  2,857.14   |
|    10      |  80            |  30          |  24,000       |  7.353s | 6.791s  | 5.861s |  3,263.97   |

Nice, we don't see any scaling problem when on constant number of agents when they are busy with more tasks.
Again we seem to have come to the same conclusion that the library scales well, the degradation is our unrealisitic approach of rollback and retrying when there is contention on branches, in real life. this is a merge and that doesn't happen in real time, unless we have some solid logic behind it, which we currently do not.

This final benchmark can put our minds to rest, we have the means to minimize contention on branches, 
each agent is choosing a branch for each commit randomlly, if we increase the branch pool the likelyhood of contention is decreased, and we can do that via the `-b` switch, let do that while keeping constant number of actions.
Let's also choose 20 agents, the number where we saw the problem start to bud.

| Num agents | Total Actions |  Num branches|   real  |  user   |  sys   | Actions/sec |
|:----------:|:-------------:|:------------:|:-------:|:-------:|:------:|:-----------:|
|    20      |  30,000       |  10          | 17.038s | 16.214s | 13.051s|  1,760.77   |
|    20      |  30,000       |  20          | 11.564s |  9.700s |  9.277s|  2,594.25   |
|    20      |  30,000       |  40          |  8.162s |  6.537s  | 6.729s|  3,675.56   |

We have seen this more than 3000 actions per second before, with lower number of actions and agents, This seems to support out above assumption that contention/rollback/retries are responsible for the above degradation.

#### Summary

The `greens` utility helped us verify that most imporantly, the result of competitng agents introdcing data to the repository is correct and valid, but it also shows that, merging and conflicts notwithstanding, it scales predictably and well. also >3000 actions per second on a medium tier laptop is more than what I expected, to be honest.

## Intresting future development

C⊕rdoba may be enough for the original requirements, but it's not nearly as feature full like Dolt, some of the features are trivial to implement with a git backend, others, like queries and schema support, may have to perform some serious tradeoff jugglinga, but these features may boost the usecase the library.

* Crenditials and security
* Indexing and queries
* Schema, constrainsts  & Validations
* Enahnced merging (As function of schema)
* Data tagging (Trival, wasn't part of the discovery effort, can be performed via git cli)
* Data merging (Trival, wasn't part of the discovery effort, can be performed via git cli)
* Adding notes (Trival, wasn't part of the discovery effort, can be performed via git cli)
* Thread context, that allows continuation without the need of propogating the context down the stack call (Experimental, not fully tested)

----

## Contributing

C⊕rdoba welcomes your contributions! just is released under the maximally permissive [CC0](https://creativecommons.org/publicdomain/zero/1.0/legalcode.txt) public domain dedication and fallback license, so your changes must also be released under this license.

### Getting started

C⊕rdoba is written in modern C++ and requires g++13 or later. All new features must be covered by unit or integration tests all tests can be found at main/gd/test, and should be exeucted prior to the pull request.

----

## License

 is licensed under the MIT License. See the LICENSE file for details.

----

## Contact

For questions or feedback, please open an issue on GitHub or contact C⊕rdoba [here](cordoba@xmousse.com).
